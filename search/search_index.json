{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ComputePods Utilities Problem Core capabilities The core of a functioning ComputePod (and so of any federation of ComputePods) is an information flow between the various specialised Chefs, the MajorDomo, as well as the user's inotify2nats and either the command line cpCtrl or browser MajorDomoUI tools. The MajorDomo is responsible for maintaining a model of the dependencies between artefacts, and determining the order in which these artefacts can be (re)built. Unlike a typical Make tool, the MajorDomo must be able to rebuild projects with circular dependencies. The various specialised Chefs are each responsible for declaring dependencies between particular types of artefacts, as well as understanding how to actually (re)build artefacts. The user's inotify2nats tool is responsible for notifying a federation of ComputePods of changes to primary artefacts being made by a user in the file systems outside of any ComputePod in the federation. Finally, the user can use either the cpCtrl command line tool or the browser based MajorDomoUI to request that a particular artefact be (re)built. Communication back-planes Each of these responsibilities represent a core capability which will \"run\" in one or more tool. Most importantly, to function, these capabilities must communicate abilities and needs between themselves. For any federation of ComputePods, there are three communication channels: Between the user and the ComputePods, there is a RESTful HTTP interface (using JSON bodies). Between components of one or more ComputePods, JSON messages will be sent using a NATS publish/subscribe system over various NATS subjects. There are also the contents of files in the file system transferred between ComputePods via sftp (as needed). Goals The ComputePods Python Utilities will capture these various core capabilities in one place so that they can be tested and developed together. Solution sketch We develop the following Python modules to implement these capabilities: The FSWatcher monitors the relatively high frequency file system changes, and sends the more important changes to the MajorDomo via NATS messages. The DependencyManager maintains a possibly cyclic graph of the (build) dependencies, as well as tools to extract the current best build tree. The ArtefactManager monitors the current (build) state of the artefacts as they are spread across the ComputePods in a federation. The RulesManager is used by the ComputePod Chefs to listen for \"how to build\" messages and responde with \"can build from\" messages. The DependencyManager uses these \"rules\" to build its dependency graph. We also develop a number of \"simple\" utilities to support these tools: The restClient provides a simple interface to send (and receive) the RESTful HTTP messages (and translate these message from the \"HTTP\" \"wire\" format into the corresponding JSON structures). The natsClient provides a simple interface to send and receive NATS messages (and translate these messages from the \"wire\" format into the corresponding JSON structures). The sftpClient provides a simple interface to move files between ComputePods. Thoughts Computing SHA256 of files... Whoever computes these SHA256 needs to keep an internal model of the file system so that they do not have to bother shelling out to compute the SHA256 on an non-existent file. I guess we could use aiofiles.os or aioshutil to check for the existence of a file before we shell-out. THE primary problem will be computing the SHA256 on a file which is being continuously written to... How do we detect that? We can do this with a debounce technique: https://developpaper.com/throttle-function-and-debounce-function/ However debouncing will introduce a latency... how much latency will ComputePods tolerate? When does the ComputePods want to know that a file has \"changed\"? Are there files that need to be tracked closely? AND files for which it does not matter? If there are... how do we signal that? A ComputePod federation needs to know whether or not to (re)start a \"compilation\" on a given file. That is whether or not a files dependants need to be recomputed. To do this, what information does a ComputePod need to know about a file? open/closed/moved (last) modification time file size sha256 do we care about permissions number of links uid/gid Is it good enough to only compute a SHA256 if the modification time has changed but the size have not changed? This suggests that, really, a ComputePod Chef should compute the SHA256 values and only just before committing to a \"re-compile\"... How often does a ComputePod federation want to be told about a \"changed\" file? Given the relative length of time it takes for a ComputePod Chef to \"re-build\" an artefact, it probably does not care about a slight latency, AND would rather only be informed of a change if/when the file stops changing. This suggests that FSWatcher needs to keep a simple model of the file system churn , so that it can detect when the changes to a given file stop via a close event. However, log files DO want to be monitored while they are changing . How do we inform FSWatcher of this? Who can tell the difference between a \"build artefact\" and a \"log file\"? A ComputePod Chef... In a ComputePods federation, where are FSWatchers needed? in the user's common area (needed since user's file changes are not visible inside a Pod) in the MajorDomo? in each Chef?","title":"ComputePods Utilities"},{"location":"#computepods-utilities","text":"","title":"ComputePods Utilities"},{"location":"#problem","text":"","title":"Problem"},{"location":"#core-capabilities","text":"The core of a functioning ComputePod (and so of any federation of ComputePods) is an information flow between the various specialised Chefs, the MajorDomo, as well as the user's inotify2nats and either the command line cpCtrl or browser MajorDomoUI tools. The MajorDomo is responsible for maintaining a model of the dependencies between artefacts, and determining the order in which these artefacts can be (re)built. Unlike a typical Make tool, the MajorDomo must be able to rebuild projects with circular dependencies. The various specialised Chefs are each responsible for declaring dependencies between particular types of artefacts, as well as understanding how to actually (re)build artefacts. The user's inotify2nats tool is responsible for notifying a federation of ComputePods of changes to primary artefacts being made by a user in the file systems outside of any ComputePod in the federation. Finally, the user can use either the cpCtrl command line tool or the browser based MajorDomoUI to request that a particular artefact be (re)built.","title":"Core capabilities"},{"location":"#communication-back-planes","text":"Each of these responsibilities represent a core capability which will \"run\" in one or more tool. Most importantly, to function, these capabilities must communicate abilities and needs between themselves. For any federation of ComputePods, there are three communication channels: Between the user and the ComputePods, there is a RESTful HTTP interface (using JSON bodies). Between components of one or more ComputePods, JSON messages will be sent using a NATS publish/subscribe system over various NATS subjects. There are also the contents of files in the file system transferred between ComputePods via sftp (as needed).","title":"Communication back-planes"},{"location":"#goals","text":"The ComputePods Python Utilities will capture these various core capabilities in one place so that they can be tested and developed together.","title":"Goals"},{"location":"#solution-sketch","text":"We develop the following Python modules to implement these capabilities: The FSWatcher monitors the relatively high frequency file system changes, and sends the more important changes to the MajorDomo via NATS messages. The DependencyManager maintains a possibly cyclic graph of the (build) dependencies, as well as tools to extract the current best build tree. The ArtefactManager monitors the current (build) state of the artefacts as they are spread across the ComputePods in a federation. The RulesManager is used by the ComputePod Chefs to listen for \"how to build\" messages and responde with \"can build from\" messages. The DependencyManager uses these \"rules\" to build its dependency graph. We also develop a number of \"simple\" utilities to support these tools: The restClient provides a simple interface to send (and receive) the RESTful HTTP messages (and translate these message from the \"HTTP\" \"wire\" format into the corresponding JSON structures). The natsClient provides a simple interface to send and receive NATS messages (and translate these messages from the \"wire\" format into the corresponding JSON structures). The sftpClient provides a simple interface to move files between ComputePods.","title":"Solution sketch"},{"location":"#thoughts","text":"Computing SHA256 of files... Whoever computes these SHA256 needs to keep an internal model of the file system so that they do not have to bother shelling out to compute the SHA256 on an non-existent file. I guess we could use aiofiles.os or aioshutil to check for the existence of a file before we shell-out. THE primary problem will be computing the SHA256 on a file which is being continuously written to... How do we detect that? We can do this with a debounce technique: https://developpaper.com/throttle-function-and-debounce-function/ However debouncing will introduce a latency... how much latency will ComputePods tolerate? When does the ComputePods want to know that a file has \"changed\"? Are there files that need to be tracked closely? AND files for which it does not matter? If there are... how do we signal that? A ComputePod federation needs to know whether or not to (re)start a \"compilation\" on a given file. That is whether or not a files dependants need to be recomputed. To do this, what information does a ComputePod need to know about a file? open/closed/moved (last) modification time file size sha256 do we care about permissions number of links uid/gid Is it good enough to only compute a SHA256 if the modification time has changed but the size have not changed? This suggests that, really, a ComputePod Chef should compute the SHA256 values and only just before committing to a \"re-compile\"... How often does a ComputePod federation want to be told about a \"changed\" file? Given the relative length of time it takes for a ComputePod Chef to \"re-build\" an artefact, it probably does not care about a slight latency, AND would rather only be informed of a change if/when the file stops changing. This suggests that FSWatcher needs to keep a simple model of the file system churn , so that it can detect when the changes to a given file stop via a close event. However, log files DO want to be monitored while they are changing . How do we inform FSWatcher of this? Who can tell the difference between a \"build artefact\" and a \"log file\"? A ComputePod Chef... In a ComputePods federation, where are FSWatchers needed? in the user's common area (needed since user's file changes are not visible inside a Pod) in the MajorDomo? in each Chef?","title":"Thoughts"},{"location":"Architecture/ArtefactManager/","text":"Artefact Manager The ArtefactManager maintains a model of the current build state of the artefacts relevant to a federation of ComputePods. The ArtefactManager's primary goal is to identify when an artefact has changed and so all dependant artefacts need to be rebuilt. Once the ArtefactManager detects a change, then it requests the DependencyManager to rebuild any dependent artefacts required for any current build goals. The ArtefactManager must also model where in a federation, the most recently up-to-date version of the artefact is located. Questions How do we determine artefact types? By file extensions By ripgrep or the-silver-searcher probes Where do we record these artefact types? In the rules YAML maintained by the rules manager Who determines the type of an artefact? The Artefact Manager on CREATE events? (Since this will be run inside a ComputePod container so we know either ripgrep or the-silver-searcher will be installed)","title":"Artefact Manager"},{"location":"Architecture/ArtefactManager/#artefact-manager","text":"The ArtefactManager maintains a model of the current build state of the artefacts relevant to a federation of ComputePods. The ArtefactManager's primary goal is to identify when an artefact has changed and so all dependant artefacts need to be rebuilt. Once the ArtefactManager detects a change, then it requests the DependencyManager to rebuild any dependent artefacts required for any current build goals. The ArtefactManager must also model where in a federation, the most recently up-to-date version of the artefact is located.","title":"Artefact Manager"},{"location":"Architecture/ArtefactManager/#questions","text":"How do we determine artefact types? By file extensions By ripgrep or the-silver-searcher probes Where do we record these artefact types? In the rules YAML maintained by the rules manager Who determines the type of an artefact? The Artefact Manager on CREATE events? (Since this will be run inside a ComputePod container so we know either ripgrep or the-silver-searcher will be installed)","title":"Questions"},{"location":"Architecture/DependencyManager/","text":"Dependency Manager The DependencyManager maintains a possibly cyclic graph of the current build dependencies for a federation of ComputePods. The dependency manager maintains a (small) collection of build goals, and by sending out a succession of \"how to build\" messages, accumulates a dependency graph sufficient to build these goals. The \"how to build\" messages contain a list of currently known artefact types as well as a small collection of goal artefact types. In response one or more Chefs, respond with \"can build from\" messages which contain one goal artefact type together with a list of required input artefact types. Definitions Dependency : An artefact A is a dependency of artefact B if A is required to build B . The dependency relation answers the question: \"What do I need to build this\"? Dependant : An artefact A is a dependant of artefact B if when artefact B is changed, artefact A must be rebuilt. The dependant relation answers the question: \"If this is changed what needs to be rebuilt\"? Note that the dependency relation is the opposite of the dependant relation . We build the dependency graph from goals to available artefacts via the dependency relationship . We build artefacts from changed artefacts to goals via the dependant relationship . (See: tup 's architecture ) This means that the dependency graph must be implemented so that it is easy to traverse the graph in either direction. The dependency graph will be implemented as a Python dictionary which contains individual nodes each of which have at least a dependencies and a dependants properties. The dependencies and dependant properties are each lists of the respective nodes. Following tup , nodes will represent both file artefacts as well as Chef types. Dependency analysis Problem While each ComputePod worker will have default knowledge of how to build objects in its area of speciality, this will often not be enough to build a large complex project which spans the domains of multiple workers. How do we supplement the \"default\" dependency knowledge? Examples For example, ConTeXt document collections have a definite suggested structure using the ConTeXt Project structures . Unfortunately, this \"default\" directory structure (and their associated ConTeXt declarations) does not include information about what diSimplex Code objects these ConTeXt documents might produce. The names of these code objects might have no relationship with, for example, the names of the containing ConTeXt documents. This means that, by default, the ComputePod workers will have no obvious way to infer that the request to build a given Code object, requires the typesetting of the associated ConTeXt documents. Requirements We need a simple text format in which to specify these high-level dependencies. This format also needs to be readable by multiple programming/scripting languages. This text format needs to support the specification of \"simple\" rules for associating artefacts with dependencies. Generally generic and/or details of dynamically generated dependencies should be located in the worker's associated ComputePod Chef plugins (and not the \"high-level\" project descriptions). Solution We will use a YAML format based upon the Sake format to describe the dependencies in a project . This description should be sufficient to start the dependency analysis phase. We expect the worker's ComputePod Chefs to be able to fill in the fine dependency details as well as the rules required to build each (micro) step. The project description file will be located in the top-level of the project file directories. Project descriptions will not contain Python or Lua (or any Turing complete) code. They may contain wildcards describing other project files (which could be additional (sub)project description files). Potential formats Our own format This would require a parser etc... ;-( YAML This is a nice format which is easily human readable, and which allows comments. There are Python, GoLang, and ANSI-C parsers, but unfortunately there is no pure Lua parser (see lua-tinyyaml ). Generation of these high-level project description files should be fairly easy in most languages. The use of (standard) \"wildcard\" characters (' ', '%', etc) in YAML strings tends to confuse YAML parsers. Which means that users must* put \"rules\" inside quoted strings. JSON This format is machine readable but can be rather wordy, and does not allow comments. There are parsers in Python, GoLang, ANSI-C and (pure) Lua (at least in ConTeXt's version). Lua Pure Lua source code could be used as a text format. It can be embedded in both ANSI-C, Python and GoLang. Unfortunately it is rather over powered (being Turing Complete). Python Pure Python source code could be used as a text format. It could only be used in ANSI-C(?) and Python. Again, it is rather over powered. Questions What sorts of dependency rules are used in build systems? see: Sake for a YAML example see: tup for an example of a simple non-standard format. see: Rake for an example of a Ruby DSL (which is ultimately Turing complete). How should we include more complex dependency rules? Should/could we use Python or Lua code? (Is this not too overpowered and a security risk?) A : We will NOT include any Turing complete \"code\". Any such code must be contained in the worker's Chef plugins (as python code), which is \"installed\" at pod build time (rather than pod runtime).","title":"Dependency Manager"},{"location":"Architecture/DependencyManager/#dependency-manager","text":"The DependencyManager maintains a possibly cyclic graph of the current build dependencies for a federation of ComputePods. The dependency manager maintains a (small) collection of build goals, and by sending out a succession of \"how to build\" messages, accumulates a dependency graph sufficient to build these goals. The \"how to build\" messages contain a list of currently known artefact types as well as a small collection of goal artefact types. In response one or more Chefs, respond with \"can build from\" messages which contain one goal artefact type together with a list of required input artefact types. Definitions Dependency : An artefact A is a dependency of artefact B if A is required to build B . The dependency relation answers the question: \"What do I need to build this\"? Dependant : An artefact A is a dependant of artefact B if when artefact B is changed, artefact A must be rebuilt. The dependant relation answers the question: \"If this is changed what needs to be rebuilt\"? Note that the dependency relation is the opposite of the dependant relation . We build the dependency graph from goals to available artefacts via the dependency relationship . We build artefacts from changed artefacts to goals via the dependant relationship . (See: tup 's architecture ) This means that the dependency graph must be implemented so that it is easy to traverse the graph in either direction. The dependency graph will be implemented as a Python dictionary which contains individual nodes each of which have at least a dependencies and a dependants properties. The dependencies and dependant properties are each lists of the respective nodes. Following tup , nodes will represent both file artefacts as well as Chef types.","title":"Dependency Manager"},{"location":"Architecture/DependencyManager/#dependency-analysis","text":"","title":"Dependency analysis"},{"location":"Architecture/DependencyManager/#problem","text":"While each ComputePod worker will have default knowledge of how to build objects in its area of speciality, this will often not be enough to build a large complex project which spans the domains of multiple workers. How do we supplement the \"default\" dependency knowledge?","title":"Problem"},{"location":"Architecture/DependencyManager/#examples","text":"For example, ConTeXt document collections have a definite suggested structure using the ConTeXt Project structures . Unfortunately, this \"default\" directory structure (and their associated ConTeXt declarations) does not include information about what diSimplex Code objects these ConTeXt documents might produce. The names of these code objects might have no relationship with, for example, the names of the containing ConTeXt documents. This means that, by default, the ComputePod workers will have no obvious way to infer that the request to build a given Code object, requires the typesetting of the associated ConTeXt documents.","title":"Examples"},{"location":"Architecture/DependencyManager/#requirements","text":"We need a simple text format in which to specify these high-level dependencies. This format also needs to be readable by multiple programming/scripting languages. This text format needs to support the specification of \"simple\" rules for associating artefacts with dependencies. Generally generic and/or details of dynamically generated dependencies should be located in the worker's associated ComputePod Chef plugins (and not the \"high-level\" project descriptions).","title":"Requirements"},{"location":"Architecture/DependencyManager/#solution","text":"We will use a YAML format based upon the Sake format to describe the dependencies in a project . This description should be sufficient to start the dependency analysis phase. We expect the worker's ComputePod Chefs to be able to fill in the fine dependency details as well as the rules required to build each (micro) step. The project description file will be located in the top-level of the project file directories. Project descriptions will not contain Python or Lua (or any Turing complete) code. They may contain wildcards describing other project files (which could be additional (sub)project description files).","title":"Solution"},{"location":"Architecture/DependencyManager/#potential-formats","text":"Our own format This would require a parser etc... ;-( YAML This is a nice format which is easily human readable, and which allows comments. There are Python, GoLang, and ANSI-C parsers, but unfortunately there is no pure Lua parser (see lua-tinyyaml ). Generation of these high-level project description files should be fairly easy in most languages. The use of (standard) \"wildcard\" characters (' ', '%', etc) in YAML strings tends to confuse YAML parsers. Which means that users must* put \"rules\" inside quoted strings. JSON This format is machine readable but can be rather wordy, and does not allow comments. There are parsers in Python, GoLang, ANSI-C and (pure) Lua (at least in ConTeXt's version). Lua Pure Lua source code could be used as a text format. It can be embedded in both ANSI-C, Python and GoLang. Unfortunately it is rather over powered (being Turing Complete). Python Pure Python source code could be used as a text format. It could only be used in ANSI-C(?) and Python. Again, it is rather over powered.","title":"Potential formats"},{"location":"Architecture/DependencyManager/#questions","text":"What sorts of dependency rules are used in build systems? see: Sake for a YAML example see: tup for an example of a simple non-standard format. see: Rake for an example of a Ruby DSL (which is ultimately Turing complete). How should we include more complex dependency rules? Should/could we use Python or Lua code? (Is this not too overpowered and a security risk?) A : We will NOT include any Turing complete \"code\". Any such code must be contained in the worker's Chef plugins (as python code), which is \"installed\" at pod build time (rather than pod runtime).","title":"Questions"},{"location":"Architecture/FSWatcher/","text":"FSWatcher The FSWatcher is responsible for tracking the current state of a file in a file system. We do this by tracking the relatively high frequency file system change events and merging these events into a model of the current state of a given file. On any (relevant) state changes for a given file, the FSWatcher sends a NATS message informing the ComputePod of this change. For our current implementation, we do this by using Linux inotify events. This means that the current implementation is limited to running ComputePods on Linux computers. Comments Note that \"log files\" have no dependants... and so we really do not need to keep SHA256 values for these files. Anything which does have a dependant will need a SHA256 value when that file has finished changing. Really we only need SHA256 values for primary artefacts, those which will be directly edited by users. How do we determine these artefacts? I guess they have no explicit dependencies (they are implicitly dependant on a Human user's whim). NO: we also need SHA256 values for artefacts which are automatically generated by some build step. By comparing old and new SHA256 values we can determine if the artefact has in fact been changed in any substantial way. Such artefacts can be marked as such by the rules manager. Who should run the SHA256 computations? The Artefact Manager since it has a better understanding of what a given file represents, as well as when there is a need to (re)SHA256.","title":"FSWatcher"},{"location":"Architecture/FSWatcher/#fswatcher","text":"The FSWatcher is responsible for tracking the current state of a file in a file system. We do this by tracking the relatively high frequency file system change events and merging these events into a model of the current state of a given file. On any (relevant) state changes for a given file, the FSWatcher sends a NATS message informing the ComputePod of this change. For our current implementation, we do this by using Linux inotify events. This means that the current implementation is limited to running ComputePods on Linux computers.","title":"FSWatcher"},{"location":"Architecture/FSWatcher/#comments","text":"Note that \"log files\" have no dependants... and so we really do not need to keep SHA256 values for these files. Anything which does have a dependant will need a SHA256 value when that file has finished changing. Really we only need SHA256 values for primary artefacts, those which will be directly edited by users. How do we determine these artefacts? I guess they have no explicit dependencies (they are implicitly dependant on a Human user's whim). NO: we also need SHA256 values for artefacts which are automatically generated by some build step. By comparing old and new SHA256 values we can determine if the artefact has in fact been changed in any substantial way. Such artefacts can be marked as such by the rules manager. Who should run the SHA256 computations? The Artefact Manager since it has a better understanding of what a given file represents, as well as when there is a need to (re)SHA256.","title":"Comments"},{"location":"Architecture/RulesManager/","text":"Rules Manager The RulesManager maintains a collection of \"rules\" for a specific type of ComputePod Chef which define both how to build an artefact as well as what inputs are required for this build. Since Python is essentially single threaded, all actual \"builds\" will take place as distinct OS processes managed by Python asyncio subprocesses. We will use a YAML format similar to Sake 's \"patterns\" The rules manager will listen for \"how to build\" messages and respond with \"can build from\" messages for any artefact for which it has rules. It will also supply build rules to a given ComputePod Chef when the Chef determines that an artefact needs to be rebuilt.","title":"Rules Manager"},{"location":"Architecture/RulesManager/#rules-manager","text":"The RulesManager maintains a collection of \"rules\" for a specific type of ComputePod Chef which define both how to build an artefact as well as what inputs are required for this build. Since Python is essentially single threaded, all actual \"builds\" will take place as distinct OS processes managed by Python asyncio subprocesses. We will use a YAML format similar to Sake 's \"patterns\" The rules manager will listen for \"how to build\" messages and respond with \"can build from\" messages for any artefact for which it has rules. It will also supply build rules to a given ComputePod Chef when the Chef determines that an artefact needs to be rebuilt.","title":"Rules Manager"},{"location":"Details/cputils/","text":"CPUtils fsWatcher get_directories_recursive ( path ) Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. Source code in cputils/fsWatcher.py def get_directories_recursive ( path ) : ''' Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. ''' if path . is_dir () : yield path for child in path . iterdir (): yield from get_directories_recursive ( child ) elif path . is_file () : yield path","title":"CPUtils"},{"location":"Details/cputils/#cputils_1","text":"","title":"CPUtils"},{"location":"Details/cputils/#cputils.fsWatcher","text":"","title":"fsWatcher"},{"location":"Details/cputils/#cputils.fsWatcher.get_directories_recursive","text":"Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. Source code in cputils/fsWatcher.py def get_directories_recursive ( path ) : ''' Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. ''' if path . is_dir () : yield path for child in path . iterdir (): yield from get_directories_recursive ( child ) elif path . is_file () : yield path","title":"get_directories_recursive()"},{"location":"Details/Tests/fsWatcher/","text":"FSWatcher tests TestRecursiveWatch setUpClass () Hook method for setting up class fixture before running tests in the class. Source code in tests/fsWatcher_tests.py def setUpClass () : os . makedirs ( os . path . join ( cputilsTestDir , 'test01' ), exist_ok = True ) os . system ( \"tree \" + cputilsTestDir ) with open ( os . path . join ( cputilsTestDir , 'test01' , 'silly.txt' ), 'w' ) as f : f . write ( \"This is a test\" ) loggedMsgs = [] tearDownClass () Hook method for deconstructing the class fixture after running all tests in the class. Source code in tests/fsWatcher_tests.py def tearDownClass () : shutil . rmtree ( cputilsTestDir )","title":"FSWatcher tests"},{"location":"Details/Tests/fsWatcher/#fswatcher-tests","text":"","title":"FSWatcher tests"},{"location":"Details/Tests/fsWatcher/#tests.fsWatcher_tests.TestRecursiveWatch","text":"","title":"TestRecursiveWatch"},{"location":"Details/Tests/fsWatcher/#tests.fsWatcher_tests.TestRecursiveWatch.setUpClass","text":"Hook method for setting up class fixture before running tests in the class. Source code in tests/fsWatcher_tests.py def setUpClass () : os . makedirs ( os . path . join ( cputilsTestDir , 'test01' ), exist_ok = True ) os . system ( \"tree \" + cputilsTestDir ) with open ( os . path . join ( cputilsTestDir , 'test01' , 'silly.txt' ), 'w' ) as f : f . write ( \"This is a test\" ) loggedMsgs = []","title":"setUpClass()"},{"location":"Details/Tests/fsWatcher/#tests.fsWatcher_tests.TestRecursiveWatch.tearDownClass","text":"Hook method for deconstructing the class fixture after running all tests in the class. Source code in tests/fsWatcher_tests.py def tearDownClass () : shutil . rmtree ( cputilsTestDir )","title":"tearDownClass()"},{"location":"Details/Tests/rulesManager/","text":"RulesManager tests This collection of tests, tests the RulesManager component. The rules_tests module collects various tests of the RuleManager . TestRulesManager Test the Rules Manager. setUpClass () Setup the tests by creating a private directory in /tmp Source code in tests/rules_tests.py def setUpClass () : \"\"\" Setup the tests by creating a private directory in /tmp \"\"\" os . makedirs ( os . path . join ( cputilsTestDir , 'test01' ), exist_ok = True ) os . system ( \"tree \" + cputilsTestDir ) with open ( os . path . join ( cputilsTestDir , 'test01' , 'silly.txt' ), 'w' ) as f : f . write ( \"This is a test\" ) tearDownClass () Tear down the private directory in /tmp Source code in tests/rules_tests.py def tearDownClass () : \"\"\" Tear down the private directory in /tmp \"\"\" shutil . rmtree ( cputilsTestDir ) test_loadRules ( t ) When loading a rule set, we want to make sure that any new Artefact types are sent to the ArtefactManager via NATS. Source code in tests/rules_tests.py @unittest . skip ( \"No real tests yet\" ) def test_loadRules ( t ) : \"\"\" When loading a rule set, we want to make sure that any new Artefact types are sent to the ArtefactManager via NATS. \"\"\" cputils . rulesManager . loadRules ( '../examples/rulesManager' ) test_ruleManager ( t ) async Test the RuleManager Source code in tests/rules_tests.py @asyncTestOfProcess () async def test_ruleManager ( t ) : \"\"\" Test the **RuleManager** \"\"\" t . assertTrue ( True )","title":"RulesManager tests"},{"location":"Details/Tests/rulesManager/#rulesmanager-tests","text":"This collection of tests, tests the RulesManager component. The rules_tests module collects various tests of the RuleManager .","title":"RulesManager tests"},{"location":"Details/Tests/rulesManager/#tests.rules_tests.TestRulesManager","text":"Test the Rules Manager.","title":"TestRulesManager"},{"location":"Details/Tests/rulesManager/#tests.rules_tests.TestRulesManager.setUpClass","text":"Setup the tests by creating a private directory in /tmp Source code in tests/rules_tests.py def setUpClass () : \"\"\" Setup the tests by creating a private directory in /tmp \"\"\" os . makedirs ( os . path . join ( cputilsTestDir , 'test01' ), exist_ok = True ) os . system ( \"tree \" + cputilsTestDir ) with open ( os . path . join ( cputilsTestDir , 'test01' , 'silly.txt' ), 'w' ) as f : f . write ( \"This is a test\" )","title":"setUpClass()"},{"location":"Details/Tests/rulesManager/#tests.rules_tests.TestRulesManager.tearDownClass","text":"Tear down the private directory in /tmp Source code in tests/rules_tests.py def tearDownClass () : \"\"\" Tear down the private directory in /tmp \"\"\" shutil . rmtree ( cputilsTestDir )","title":"tearDownClass()"},{"location":"Details/Tests/rulesManager/#tests.rules_tests.TestRulesManager.test_loadRules","text":"When loading a rule set, we want to make sure that any new Artefact types are sent to the ArtefactManager via NATS. Source code in tests/rules_tests.py @unittest . skip ( \"No real tests yet\" ) def test_loadRules ( t ) : \"\"\" When loading a rule set, we want to make sure that any new Artefact types are sent to the ArtefactManager via NATS. \"\"\" cputils . rulesManager . loadRules ( '../examples/rulesManager' )","title":"test_loadRules()"},{"location":"Details/Tests/rulesManager/#tests.rules_tests.TestRulesManager.test_ruleManager","text":"Test the RuleManager Source code in tests/rules_tests.py @asyncTestOfProcess () async def test_ruleManager ( t ) : \"\"\" Test the **RuleManager** \"\"\" t . assertTrue ( True )","title":"test_ruleManager()"},{"location":"Details/Tests/testUtils/","text":"Test Utilities asyncTestOfProcess ( asyncProcessFunc = None ) The asyncTestOfProcess decorator expects as an argument an async coroutine which runs the long running asynchronous process to be tested. The asyncTestOfProcess decorates another async coroutine containing the tests and assertions used to test the long running process itself. Both the argument and the decorated function MUST be async coroutines, AND the process needs to place \"things\" into the t.asyncTestQueue, AND the decorated function MUST await on t.asyncTestQueue.get() to allow the tested process time to function. Source code in tests/testUtils.py def asyncTestOfProcess ( asyncProcessFunc = None ) : \"\"\" The asyncTestOfProcess **decorator** expects as an *argument* an async coroutine which runs the long running asynchronous process to be tested. The asyncTestOfProcess *decorates* another async coroutine containing the tests and assertions used to test the long running process itself. Both the argument and the decorated function MUST be async coroutines, AND the process needs to place \"things\" into the t.asyncTestQueue, AND the decorated function MUST await on t.asyncTestQueue.get() to allow the tested process time to function. \"\"\" def decorateTest ( asyncTestFunc ) : @functools . wraps ( asyncTestFunc ) def wrappedTest ( t ) : async def asyncRunner () : t . asyncTestQueue = asyncio . Queue () if asyncProcessFunc : if not asyncio . iscoroutinefunction ( asyncProcessFunc ) : t . fail ( \"The process being tested MUST be a async coroutine!\" ) processRunner = asyncio . create_task ( asyncProcessFunc ( t )) if not asyncio . iscoroutinefunction ( asyncTestFunc ) : t . fail ( \"The test being run on the process MUST be a async coroutine!\" ) asyncTestFuture = asyncio . get_event_loop () . create_future () async def wrappedAsyncTestFunc () : try : await asyncTestFunc ( t ) asyncTestFuture . set_result ( None ) except asyncio . CancelledError : pass except Exception as err : asyncTestFuture . set_exception ( err ) testRunner = asyncio . create_task ( wrappedAsyncTestFunc ()) await asyncTestFuture try : asyncio . run ( asyncRunner ()) except asyncio . CancelledError : pass return wrappedTest return decorateTest","title":"Test Utilities"},{"location":"Details/Tests/testUtils/#test-utilities","text":"","title":"Test Utilities"},{"location":"Details/Tests/testUtils/#tests.testUtils.asyncTestOfProcess","text":"The asyncTestOfProcess decorator expects as an argument an async coroutine which runs the long running asynchronous process to be tested. The asyncTestOfProcess decorates another async coroutine containing the tests and assertions used to test the long running process itself. Both the argument and the decorated function MUST be async coroutines, AND the process needs to place \"things\" into the t.asyncTestQueue, AND the decorated function MUST await on t.asyncTestQueue.get() to allow the tested process time to function. Source code in tests/testUtils.py def asyncTestOfProcess ( asyncProcessFunc = None ) : \"\"\" The asyncTestOfProcess **decorator** expects as an *argument* an async coroutine which runs the long running asynchronous process to be tested. The asyncTestOfProcess *decorates* another async coroutine containing the tests and assertions used to test the long running process itself. Both the argument and the decorated function MUST be async coroutines, AND the process needs to place \"things\" into the t.asyncTestQueue, AND the decorated function MUST await on t.asyncTestQueue.get() to allow the tested process time to function. \"\"\" def decorateTest ( asyncTestFunc ) : @functools . wraps ( asyncTestFunc ) def wrappedTest ( t ) : async def asyncRunner () : t . asyncTestQueue = asyncio . Queue () if asyncProcessFunc : if not asyncio . iscoroutinefunction ( asyncProcessFunc ) : t . fail ( \"The process being tested MUST be a async coroutine!\" ) processRunner = asyncio . create_task ( asyncProcessFunc ( t )) if not asyncio . iscoroutinefunction ( asyncTestFunc ) : t . fail ( \"The test being run on the process MUST be a async coroutine!\" ) asyncTestFuture = asyncio . get_event_loop () . create_future () async def wrappedAsyncTestFunc () : try : await asyncTestFunc ( t ) asyncTestFuture . set_result ( None ) except asyncio . CancelledError : pass except Exception as err : asyncTestFuture . set_exception ( err ) testRunner = asyncio . create_task ( wrappedAsyncTestFunc ()) await asyncTestFuture try : asyncio . run ( asyncRunner ()) except asyncio . CancelledError : pass return wrappedTest return decorateTest","title":"asyncTestOfProcess()"}]}